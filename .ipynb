{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    " # change into the folder that contains the unzipped data (in the folder \"DataManagementIntergration_Data\")\n",
    "\n",
    "data_path = r'C:\\Users\\sjants\\Desktop\\Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get overview of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_folder_file = {} # initialize dictionary\n",
    "# this will hold subfolders as keys and a list of the contained files as value, e.g. dict = {folder1: [file1, file2], ...}\n",
    "\n",
    "for subfolder in os.listdir(data_path):\n",
    "    if subfolder not in dict_folder_file.keys(): # check if the dictionary already contains entries for the subfolder\n",
    "        dict_folder_file[subfolder] = [] # if not, add an empty list to as value for that entry\n",
    "    for entry in os.listdir('/'.join((data_path, subfolder))):\n",
    "        if entry == 'UnityDataSave': # if the subfolder is UnityDataSave\n",
    "            contents = os.listdir('/'.join((data_path, subfolder, entry))) # get the contents of the folder\n",
    "            contents = ['/'.join((entry, i)) for i in contents] # construct the path to the file\n",
    "            for entr in contents:\n",
    "                dict_folder_file[subfolder].append(entr) # append 'UnityDataSave/filename' to the dictionary\n",
    "        else:\n",
    "            dict_folder_file[subfolder].append(entry) # append the list for that entry with the respective files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Read in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detailed_subj_15_corsi.csv\n",
      "detailed_subj_16_corsi.csv\n",
      "detailed_subj_17_corsi.csv\n",
      "detailed_subj_18_corsi.csv\n",
      "detailed_subj_19_corsi.csv\n",
      "detailed_subj_20_corsi.csv\n",
      "detailed_subj_21_corsi.csv\n",
      "detailed_subj_22_corsi.csv\n",
      "detailed_subj_23_corsi.csv\n",
      "detailed_subj_24_corsi.csv\n",
      "detailed_subj_25_corsi.csv\n",
      "detailed_subj_26_corsi.csv\n",
      "detailed_subj_27_corsi.csv\n",
      "detailed_subj_28_corsi.csv\n",
      "detailed_subj_29_corsi.csv\n",
      "detailed_subj_30_corsi.csv\n",
      "detailed_subj_31_corsi.csv\n",
      "detailed_subj_32_corsi.csv\n",
      "detailed_subj_34_corsi.csv\n",
      "detailed_subj_35_corsi.csv\n",
      "detailed_subj_36_corsi.csv\n",
      "detailed_subj_37_corsi.csv\n",
      "detailed_subj_38_corsi.csv\n",
      "detailed_subj_39_corsi.csv\n",
      "detailed_subj_40_corsi.csv\n",
      "detailed_subj_41_corsi.csv\n",
      "detailed_subj_42_corsi.csv\n",
      "detailed_subj_43_corsi.csv\n",
      "detailed_subj_44_corsi.csv\n"
     ]
    }
   ],
   "source": [
    "for key in dict_folder_file.keys(): # iterate through each subfolder\n",
    "    print(dict_folder_file[key][0]) # print first file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['detailed_subj_15_corsi.csv', 'ptsot_results-15.txt', 'sbsod_15_Female_Age_27_20201014_094327.csv', 'UnityDataSave/15_JRD_20201014_103147.csv', 'UnityDataSave/15_UnityLOG_20201014_103147.csv']\n",
      "['detailed_subj_16_corsi.csv', 'ptsot_results-16.txt', 'sbsod_16_Male_Age_27_20201014_132539.csv', 'UnityDataSave/16_JRD_20201014_135224.csv', 'UnityDataSave/16_UnityLOG_20201014_135224.csv']\n",
      "['detailed_subj_17_corsi.csv', 'ptsot_results-17.txt', 'sbsod_17_Male_Age_21_20201014_161320.csv', 'UnityDataSave/17_JRD_20201014_170118.csv', 'UnityDataSave/17_UnityLOG_20201014_170118.csv']\n",
      "['detailed_subj_18_corsi.csv', 'ptsot_results-18.txt', 'sbsod_18_Male_Age_21_20201015_095434.csv', 'UnityDataSave/18_JRD_20201015_101733.csv', 'UnityDataSave/18_UnityLOG_20201015_101733.csv']\n",
      "['detailed_subj_19_corsi.csv', 'ptsot_results-19.txt', 'sbsod_19_Female_Age_29_20201015_134843.csv', 'UnityDataSave/1919_JRD_20201015_143137.csv', 'UnityDataSave/1919_UnityLOG_20201015_143137.csv', 'UnityDataSave/19_JRD_20201015_141249.csv', 'UnityDataSave/19_UnityLOG_20201015_141249.csv']\n",
      "['detailed_subj_20_corsi.csv', 'ptsot_results-20.txt', 'sbsod_20_Female_Age_25_20201015_162951.csv', 'UnityDataSave/20_JRD_20201015_170831.csv', 'UnityDataSave/20_UnityLOG_20201015_170831.csv']\n",
      "['detailed_subj_21_corsi.csv', 'ptsotresults-21.txt', 'sbsod_21_Female_Age_28_20201019_142007.csv', 'UnityDataSave/21_JRD_20201019_145256.csv', 'UnityDataSave/21_UnityLOG_20201019_145256.csv']\n",
      "['detailed_subj_22_corsi.csv', 'ptsotresults-22.txt', 'sbsod_22_Male_Age_24_20201021_095345.csv', 'UnityDataSave/22_JRD_20201021_102947.csv', 'UnityDataSave/22_UnityLOG_20201021_102947.csv']\n",
      "['detailed_subj_23_corsi.csv', 'ptsotresults-23.txt', 'sbsod_23_Male_Age_27_20201021_132806.csv', 'UnityDataSave/23_JRD_20201021_135337.csv', 'UnityDataSave/23_UnityLOG_20201021_135337.csv']\n",
      "['detailed_subj_24_corsi.csv', 'ptsot_results-24.txt', 'sbsod_24_Female_Age_21_20201022_095150.csv', 'UnityDataSave/24_JRD_20201022_102246.csv', 'UnityDataSave/24_UnityLOG_20201022_102246.csv']\n",
      "['detailed_subj_25_corsi.csv', 'ptsot_results-25.txt', 'sbsod_25_Male_Age_25_20201022_133608.csv', 'UnityDataSave/25_JRD_20201022_135823.csv', 'UnityDataSave/25_UnityLOG_20201022_135823.csv']\n",
      "['detailed_subj_26_corsi.csv', 'ptsotresults-26.txt', 'sbsod_26_Female_Age_26_20201022_161454.csv', 'UnityDataSave/26_JRD_20201022_164458.csv', 'UnityDataSave/26_UnityLOG_20201022_164458.csv']\n",
      "['detailed_subj_27_corsi.csv', 'ptsot_results-27.txt', 'sbsod_27_Female_Age_26_20201026_142115.csv', 'UnityDataSave/27_JRD_20201026_144926.csv', 'UnityDataSave/27_UnityLOG_20201026_144926.csv']\n",
      "['detailed_subj_28_corsi.csv', 'ptsot_results-28.txt', 'sbsob_28_Male_Age_29_20201028_094329.csv', 'UnityDataSave/28_JRD_20201028_102815.csv', 'UnityDataSave/28_RouteTest_20201028_102815.csv', 'UnityDataSave/28_UnityLOG_20201028_102815.csv']\n",
      "['detailed_subj_29_corsi.csv', 'ptsot_results-29.txt', 'sbsod_29_Female_Age_22_20201028_135201.csv', 'UnityDataSave/29_JRD_20201028_142814.csv', 'UnityDataSave/29_RouteTest_20201028_142814.csv', 'UnityDataSave/29_UnityLOG_20201028_142814.csv']\n",
      "['detailed_subj_30_corsi.csv', 'ptsot_results-30.txt', 'sbsod_30_Male_Age_23_20201029_095420.csv', 'UnityDataSave/30_JRD_20201029_102655.csv', 'UnityDataSave/30_RouteTest_20201029_102655.csv', 'UnityDataSave/30_UnityLOG_20201029_102655.csv']\n",
      "['detailed_subj_31_corsi.csv', 'ptsot_results-31.txt', 'sbsod_31_Male_Age_28_20201029_133503.csv', 'UnityDataSave/31_JRD_20201029_140128.csv', 'UnityDataSave/31_RouteTest_20201029_140128.csv', 'UnityDataSave/31_UnityLOG_20201029_140128.csv']\n",
      "['detailed_subj_32_corsi.csv', 'ptsot_results-32.txt', 'sbsod_32_Female_Age_18_20201029_162415.csv']\n",
      "['detailed_subj_34_corsi.csv', 'ptsot_results-34.txt', 'sbsod_34_Female_Age_24_20201030_134051.csv', 'UnityDataSave/34_JRD_20201030_140646.csv', 'UnityDataSave/34_RouteTest_20201030_140646.csv', 'UnityDataSave/34_UnityLOG_20201030_140646.csv']\n",
      "['detailed_subj_35_corsi.csv', 'ptsot_results-35.txt', 'sbsod_35_Female_Age_26_20201102_141805.csv']\n",
      "['detailed_subj_36_corsi.csv', 'ptsot_results-36.txt', 'sbsod_36_Male_Age_27_20201103_142218.csv', 'UnityDataSave/36_JRD_20201103_145948.csv', 'UnityDataSave/36_RouteTest_20201103_145948.csv', 'UnityDataSave/36_UnityLOG_20201103_145948.csv']\n",
      "['detailed_subj_37_corsi.csv', 'ptsot_results-37.txt', 'sbsod_37_Female_Age_27_20201105_162224.csv', 'UnityDataSave/37_JRD_20201105_170101.csv', 'UnityDataSave/37_RouteTest_20201105_170101.csv', 'UnityDataSave/37_UnityLOG_20201105_170101.csv']\n",
      "['detailed_subj_38_corsi.csv', 'ptsot_results-38.txt', 'sbsod_38_Female_Age_24_20201111_133333.csv', 'UnityDataSave/38_JRD_20201111_140336.csv', 'UnityDataSave/38_RouteTest_20201111_140336.csv', 'UnityDataSave/38_UnityLOG_20201111_140336.csv']\n",
      "['detailed_subj_39_corsi.csv', 'ptsot_results-39.txt', 'sbsod_39_Female_Age_28_20201111_162148.csv', 'UnityDataSave/39_JRD_20201111_165209.csv', 'UnityDataSave/39_RouteTest_20201111_165209.csv', 'UnityDataSave/39_UnityLOG_20201111_165209.csv']\n",
      "['detailed_subj_40_corsi.csv', 'ptsot_results-40.txt', 'sbsod_40_Female_Age_22_20201112_094903.csv', 'UnityDataSave/40_JRD_20201112_101412.csv', 'UnityDataSave/40_RouteTest_20201112_101412.csv', 'UnityDataSave/40_UnityLOG_20201112_101412.csv']\n",
      "['detailed_subj_41_corsi.csv', 'ptsot_results-41.txt', 'sbsod_41_Male_Age_23_20201112_140300.csv', 'UnityDataSave/41_JRD_20201112_142836.csv', 'UnityDataSave/41_RouteTest_20201112_142836.csv', 'UnityDataSave/41_UnityLOG_20201112_142836.csv']\n",
      "['detailed_subj_42_corsi.csv', 'ptsot_results-42.txt', 'sbsod_42_Female_Age_31_20201112_162151.csv', 'UnityDataSave/42_JRD_20201112_164559.csv', 'UnityDataSave/42_RouteTest_20201112_164559.csv', 'UnityDataSave/42_UnityLOG_20201112_164559.csv']\n",
      "['detailed_subj_43_corsi.csv', 'ptsot_results-43.txt', 'sbsod_43_Male_Age_21_20201113_095503.csv', 'UnityDataSave/43_JRD_20201113_101822.csv', 'UnityDataSave/43_RouteTest_20201113_101822.csv', 'UnityDataSave/43_UnityLOG_20201113_101822.csv']\n",
      "['detailed_subj_44_corsi.csv', 'ptsot_results-44.txt', 'sbsod_44_Female_Age_22_20201113_161422.csv', 'UnityDataSave/44_JRD_20201113_164148.csv', 'UnityDataSave/44_RouteTest_20201113_164148.csv', 'UnityDataSave/44_UnityLOG_20201113_164148.csv']\n"
     ]
    }
   ],
   "source": [
    "for key in dict_folder_file.keys(): # iterate through each subfolder\n",
    "    print(dict_folder_file[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj15\n",
      "subj16\n",
      "subj17\n",
      "subj18\n",
      "subj19\n",
      "subj20\n",
      "subj21\n",
      "subj22\n",
      "subj23\n",
      "subj24\n",
      "subj25\n",
      "subj26\n",
      "subj27\n",
      "subj28\n",
      "subj29\n",
      "subj30\n",
      "subj31\n",
      "subj32\n",
      "subj34\n",
      "subj35\n",
      "subj36\n",
      "subj37\n",
      "subj38\n",
      "subj39\n",
      "subj40\n",
      "subj41\n",
      "subj42\n",
      "subj43\n",
      "subj44\n"
     ]
    }
   ],
   "source": [
    "for key in dict_folder_file.keys(): # iterate through each subfolder\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = []\n",
    "for key in dict_folder_file.keys(): #iterate through each subfolder\n",
    "    df=pd.read_csv('/'.join((data_path,key,dict_folder_file[key][0]))) #read in one file as data frame\n",
    "    list.append(df) #append data frames\n",
    "detailed_subj = pd.concat(list, axis=0, ignore_index=True) #concatenate them into one data frame called detailed_subj\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      UserId    Correct    userAnswer    expectAnswer    curClickNumber   \\\n",
      "0          15          1             5               5                 1   \n",
      "1          15          1             6               6                 2   \n",
      "2          15          1             4               4                 3   \n",
      "3          15          1             1               1                 4   \n",
      "4          15          1             8               8                 5   \n",
      "...       ...        ...           ...             ...               ...   \n",
      "5882       44          1             0               0                 1   \n",
      "5883       44          1             5               5                 2   \n",
      "5884       44          1             2               2                 3   \n",
      "5885       44          1             7               7                 4   \n",
      "5886       44          1             6               6                 5   \n",
      "\n",
      "       expectMaxClickNumber  \n",
      "0                         7  \n",
      "1                         7  \n",
      "2                         7  \n",
      "3                         7  \n",
      "4                         7  \n",
      "...                     ...  \n",
      "5882                      5  \n",
      "5883                      5  \n",
      "5884                      5  \n",
      "5885                      5  \n",
      "5886                      5  \n",
      "\n",
      "[5887 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(detailed_subj) #print data of all subjects from the detailed_subj files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     UserID          QuestionNumber  CorrectResponseAngle  \\\n",
      "0        15                       1                 123.0   \n",
      "1        15                       2                 237.0   \n",
      "2        15                       3                  83.0   \n",
      "3        15                       4                 156.0   \n",
      "4        15                       5                 319.0   \n",
      "..      ...                     ...                   ...   \n",
      "372      44                       9                 280.0   \n",
      "373      44                      10                  48.0   \n",
      "374      44                      11                  26.0   \n",
      "375      44                      12                 150.0   \n",
      "376      44  Average Error: 24.8548                   NaN   \n",
      "\n",
      "     ActualResponseAngle  AbsoluteAngularError  \n",
      "0               136.6703               13.6703  \n",
      "1               215.3757               21.6243  \n",
      "2                89.3614                6.3614  \n",
      "3               163.9047                7.9047  \n",
      "4               328.1509                9.1509  \n",
      "..                   ...                   ...  \n",
      "372             300.3780               20.3780  \n",
      "373              69.8174               21.8174  \n",
      "374              20.7285                5.2715  \n",
      "375             133.5872               16.4128  \n",
      "376                  NaN                   NaN  \n",
      "\n",
      "[377 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "list = []\n",
    "for key in dict_folder_file.keys(): \n",
    "    df=pd.read_csv('/'.join((data_path,key,dict_folder_file[key][1])),names=['QuestionNumber','CorrectResponseAngle','ActualResponseAngle','AbsoluteAngularError'],header=None)\n",
    "    ID=[]\n",
    "    for i in range(0,len(df)):\n",
    "        ID.append(int(key[4:]))\n",
    "    df.insert(0,'UserID',ID)\n",
    "    list.append(df)\n",
    "ptsot_results = pd.concat(list, axis=0, ignore_index=True,sort=False)\n",
    "print(ptsot_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      PartID             DateTime   City_mapLMs  StandingAt  LookingAt   \\\n",
      "0     PartID             DateTime   City_mapLMs    LandMark       Seen    \n",
      "1          15  14.10.2020 10:41:25           1_7         LM5        LM6   \n",
      "2          15  14.10.2020 10:41:45           1_7         LM2        LM6   \n",
      "3          15  14.10.2020 10:42:14           1_7         LM4        LM6   \n",
      "4          15  14.10.2020 10:42:34           1_7         LM1        LM3   \n",
      "...       ...                  ...           ...         ...        ...   \n",
      "1078       44  13.11.2020 17:19:42           3_7         LM3        LM5   \n",
      "1079       44  13.11.2020 17:19:47           3_7         LM6        LM4   \n",
      "1080       44  13.11.2020 17:19:48           3_7         LM5        LM4   \n",
      "1081       44  13.11.2020 17:19:56           3_7         LM3        LM2   \n",
      "1082       44  13.11.2020 17:19:58           3_7         LM6        LM5   \n",
      "\n",
      "     LandmarkToMove  CorrectAngle   ResponseInDegrees   ResponsErrorAbs  \n",
      "0          Response   CorrectAngle                 NaN              NaN  \n",
      "1                LM7      38.28455          153.187000        114.90240  \n",
      "2                LM5      7.510761           43.030730         35.51997  \n",
      "3                LM1       256.835          213.094600         43.74046  \n",
      "4                LM2      28.29682          337.270100         51.02676  \n",
      "...              ...           ...                 ...              ...  \n",
      "1078             LM7       14.5494            3.732508         10.81691  \n",
      "1079             LM3       335.241           73.154510         97.91312  \n",
      "1080             LM6       177.012           79.987100         97.02465  \n",
      "1081             LM1       62.8291          263.248100        159.58100  \n",
      "1082             LM1       325.882          264.986100         60.89554  \n",
      "\n",
      "[1083 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "list = []\n",
    "for key in dict_folder_file.keys(): \n",
    "        if len(dict_folder_file[key]) > 3:\n",
    "            df=pd.read_csv('/'.join((data_path,key,dict_folder_file[key][3])),skipinitialspace=True)\n",
    "            list.append(df)\n",
    "JRD = pd.concat(list, axis=0, ignore_index=True,sort=False)\n",
    "print(JRD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     UserID  questionID                                    questionContent   \\\n",
      "0        16            1               I am very good at giving directions.   \n",
      "1        16            2      I have a poor memory for where I left things.   \n",
      "2        16            3               I am very good at judging distances.   \n",
      "3        16            4              My 'sense of direction' is very good.   \n",
      "4        16            5  I tend to think of my environment in terms of ...   \n",
      "..      ...          ...                                                ...   \n",
      "385      44           11            Ich gebe nicht gerne Wegbeschreibungen.   \n",
      "386      44           12  FÃ¼r mich ist es nicht wichtig; zu wissen wo i...   \n",
      "387      44           13  Normalerweise Ã¼berlasse ich anderen die Wegep...   \n",
      "388      44           14  In der Regel kann ich mich an einen neuen Weg ...   \n",
      "389      44           15  Ich habe keine sehr gute â€žinnere Karteâ€œ me...   \n",
      "\n",
      "      ParticipantAnswer  \n",
      "0                     5  \n",
      "1                     4  \n",
      "2                     4  \n",
      "3                     5  \n",
      "4                     5  \n",
      "..                  ...  \n",
      "385                   6  \n",
      "386                   3  \n",
      "387                   7  \n",
      "388                   5  \n",
      "389                   2  \n",
      "\n",
      "[390 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "list=[]\n",
    "for key in dict_folder_file.keys(): \n",
    "        if len(dict_folder_file[key]) > 3:\n",
    "            with open('/'.join((data_path,key,dict_folder_file[key][2]))) as file:\n",
    "                data = file.read().replace(\"merken,\",\"merken;\" ).replace(\"nachdenke,\",\"nachdenke;\" ).replace(\"(N,S,O,W)\",\"(N;S;O;W)\" ).replace(\"(N, S, E, W)\",\"(N; S; E; W)\" ).replace(\"Probleme,\",\"Probleme;\" ).replace(\"wichtig,\",\"wichtig;\" ).replace(\"erinnern,\",\"erinnern;\" )\n",
    "                if len(data)>0:\n",
    "                    TESTDATA = StringIO(data)\n",
    "                    df = pd.read_csv(TESTDATA, sep=\",\")\n",
    "                    ID=[]\n",
    "                    for i in range(0,len(df)):\n",
    "                        ID.append(int(key[4:]))\n",
    "                    df.insert(0,'UserID',ID)\n",
    "                    list.append(df)\n",
    "sbsod = pd.concat(list, axis=0, ignore_index=True,sort=False)\n",
    "print(sbsod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PartID             DateTime  City_mapLMs         LandMark   Seen   \\\n",
      "0         28  28.10.2020 10:35:54          1_7  NovelLM_7_0 (1)  False   \n",
      "1         28  28.10.2020 10:36:05          1_7              LM5  False   \n",
      "2         28  28.10.2020 10:36:08          1_7      NovelLM_6_0  False   \n",
      "3         28  28.10.2020 10:36:10          1_7  NovelLM_3_0 (1)  False   \n",
      "4         28  28.10.2020 10:36:22          1_7              LM1   True   \n",
      "..       ...                  ...          ...              ...    ...   \n",
      "583       44  13.11.2020 17:17:55          3_7              LM5   True   \n",
      "584       44  13.11.2020 17:17:59          3_7              LM3  False   \n",
      "585       44  13.11.2020 17:18:02          3_7      NovelLM_4_0  False   \n",
      "586       44  13.11.2020 17:18:03          3_7      NovelLM_2_0  False   \n",
      "587       44  13.11.2020 17:18:06          3_7              LM4   True   \n",
      "\n",
      "    Response  CorrectAngle  \n",
      "0         NaN       fakeLM  \n",
      "1         NaN     23.08778  \n",
      "2         NaN       fakeLM  \n",
      "3         NaN       fakeLM  \n",
      "4    Forwards        Start  \n",
      "..        ...          ...  \n",
      "583  Forwards     23.08778  \n",
      "584       NaN     75.32509  \n",
      "585       NaN       fakeLM  \n",
      "586       NaN       fakeLM  \n",
      "587      Left     317.8529  \n",
      "\n",
      "[588 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "list = []\n",
    "for key in dict_folder_file.keys(): \n",
    "        if len(dict_folder_file[key]) == 6:\n",
    "            df=pd.read_csv('/'.join((data_path,key,dict_folder_file[key][4])),skipinitialspace=True)\n",
    "            list.append(df)\n",
    "RouteTest = pd.concat(list, axis=0, ignore_index=True,sort=False)\n",
    "print(RouteTest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
